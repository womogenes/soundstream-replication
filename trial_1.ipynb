{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal convolution modules from https://github.com/wesbz/SoundStream/blob/main/net.py\n",
    "# Because I could do the math but don't want to\n",
    "\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    \"\"\"\n",
    "    1D convolution with padding at start only\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.causal_padding = self.dilation[0] * (self.kernel_size[0] - 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._conv_forward(F.pad(x, [self.causal_padding, 0]), self.weight, self.bias)\n",
    "\n",
    "class CausalConvTranspose1d(nn.ConvTranspose1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.causal_padding = self.dilation[0] * (self.kernel_size[0] - 1) + self.output_padding[0] + 1 - self.stride[0]\n",
    "    \n",
    "    def forward(self, x, output_size=None):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose1d')\n",
    "\n",
    "        assert isinstance(self.padding, tuple)\n",
    "        output_padding = self._output_padding(\n",
    "            x, output_size, self.stride, self.padding, self.kernel_size, self.dilation)\n",
    "        return F.conv_transpose1d(\n",
    "            x, self.weight, self.bias, self.stride, self.padding,\n",
    "            output_padding, self.groups, self.dilation)[...,:-self.causal_padding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self, N, dilation):\n",
    "        \"\"\"\n",
    "        N is the number of channels (stays constant)\n",
    "        dilation is Conv1d dilation for first convolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = CausalConv1d(in_channels=N, out_channels=N, kernel_size=7,\n",
    "                                  dilation=dilation)\n",
    "        self.conv2 = CausalConv1d(in_channels=N, out_channels=N, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.elu(self.conv1(x))\n",
    "        out = F.elu(x + self.conv2(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder architecture\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, N, S):\n",
    "        \"\"\"\n",
    "        N is the number of output channels (stays constant)\n",
    "            We assume the number of input channels is N/2\n",
    "        S is the stride size for downsampling\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.resunits = nn.Sequential(ResidualUnit(N//2, dilation=1),\n",
    "                                      ResidualUnit(N//2, dilation=3),\n",
    "                                      ResidualUnit(N//2, dilation=9))\n",
    "        self.conv1 = CausalConv1d(in_channels=N//2, out_channels=N, kernel_size=(2*S), stride=S)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.resunits(x)\n",
    "        out = F.elu(self.conv1(out))\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        \"\"\"\n",
    "        C is the number of channels initially\n",
    "        D is the dimensionality of the encoded vectors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = CausalConv1d(in_channels=1, out_channels=C, kernel_size=7)\n",
    "        self.encblocks = nn.Sequential(EncoderBlock(N=2*C, S=2),\n",
    "                                       EncoderBlock(N=4*C, S=4),\n",
    "                                       EncoderBlock(N=8*C, S=5),\n",
    "                                       EncoderBlock(N=16*C, S=8))\n",
    "        self.conv2 = CausalConv1d(in_channels=16*C, out_channels=D, kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.elu(self.conv1(x))\n",
    "        out = self.encblocks(out)\n",
    "        out = F.elu(self.conv2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder architecture\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, N, S):\n",
    "        \"\"\"\n",
    "        N is number of channels (assumed to stay constant)\n",
    "        S is stride size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.convT1 = CausalConvTranspose1d(in_channels=N, out_channels=N//2, kernel_size=2*S,\n",
    "                                            stride=S)\n",
    "        self.resunits = nn.Sequential(\n",
    "            ResidualUnit(N//2, dilation=1),\n",
    "            ResidualUnit(N//2, dilation=3),\n",
    "            ResidualUnit(N//2, dilation=9)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.elu(self.convT1(x))\n",
    "        out = self.resunits(out)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        \"\"\"\n",
    "        C is \"channel scale\"\n",
    "        D is dimensionality of input embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = CausalConv1d(in_channels=D, out_channels=16*C, kernel_size=7)\n",
    "        self.decblocks = nn.Sequential(\n",
    "            DecoderBlock(N=16*C, S=8),\n",
    "            DecoderBlock(N=8*C, S=5),\n",
    "            DecoderBlock(N=4*C, S=4),\n",
    "            DecoderBlock(N=2*C, S=2),\n",
    "        )\n",
    "        self.conv2 = CausalConv1d(in_channels=C, out_channels=1, kernel_size=7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x is the embeddings\"\"\"\n",
    "        out = F.elu(self.conv1(x))\n",
    "        out = self.decblocks(out)\n",
    "        out = F.elu(self.conv2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundStream(nn.Module):\n",
    "    def __init__(self, C_enc, C_dec, D):\n",
    "        \"\"\"\n",
    "        C_enc: \"channel scale\" for encoder\n",
    "        C_dec: \"channel scale\" for decoder\n",
    "        D:     dimensionality of embeddings\n",
    "        \"\"\"\n",
    "        self.encoder = Encoder(C_enc, D)\n",
    "        self.decoder = Decoder(C_dec, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a waveform of shape (batch_size, 1, sequence_length)\n",
    "        \"\"\"\n",
    "        # This will be more complicated once we get the RVQ working\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # https://stackoverflow.com/questions/42720627/python-os-walk-to-certain-level\n",
    "        # Get a list of wav file paths\n",
    "        self.wav_files = []\n",
    "        data_dir = \"data/LibriTTS/dev-clean\"\n",
    "        for root, dirs, files in tqdm(os.walk(\"data/LibriTTS/dev-clean\")):\n",
    "            if root[len(data_dir):].count(os.sep) != 2:\n",
    "                continue\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    self.wav_files.append(os.path.join(root, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torchaudio.load(self.wav_files[idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "137it [00:00, 2408.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = AudioDataset()\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
